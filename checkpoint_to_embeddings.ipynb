{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q tensorflow==1.15.0 tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bert'...\n",
      "remote: Enumerating objects: 340, done.\u001b[K\n",
      "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
      "Receiving objects: 100% (340/340), 317.84 KiB | 260.00 KiB/s, done.\n",
      "Resolving deltas: 100% (185/185), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/google-research/bert.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/camila/Desktop/projetos/ibti/myvenv/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "1.15.0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow.compat.v1 as tf\n",
    "from bert import modeling\n",
    "import shutil\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(config_path, meta_path, ckpt_path, export_dir):\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        tf.random.set_random_seed(44)\n",
    "        # these names are important, we look for these in Spark NLP when we feed the BERT model\n",
    "        bert_inputs = dict(\n",
    "            input_ids=tf.placeholder(dtype=tf.int32, shape=(None, None), name=\"input_ids\"),\n",
    "            input_mask=tf.placeholder(dtype=tf.int32, shape=(None, None), name=\"input_mask\"),\n",
    "            segment_ids=tf.placeholder(dtype=tf.int32, shape=(None, None), name=\"segment_ids\")\n",
    "        )\n",
    "\n",
    "        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                              log_device_placement=False)) as sess:\n",
    "\n",
    "            with tf.device('/gpu:0'):\n",
    "\n",
    "                bert_config = modeling.BertConfig.from_json_file(config_path)\n",
    "\n",
    "                model = modeling.BertModel(\n",
    "                    config=bert_config,\n",
    "                    is_training=False,\n",
    "                    input_ids=bert_inputs['input_ids'],\n",
    "                    input_mask=bert_inputs['input_mask'],\n",
    "                    token_type_ids=bert_inputs['segment_ids'],\n",
    "                    use_one_hot_embeddings=False\n",
    "                )\n",
    "\n",
    "                # this name is important, we look for this when we want to fetch the result\n",
    "                # as you already guessed, you can do whatever you want within the TensorFlow with this output\n",
    "                # as long as the result is DT_FLOAT with the shape of (-1, -1, 768) you can use the same name \n",
    "                # and access the results in Spark NLP               \n",
    "                sequence_output = tf.identity(model.get_sequence_output(), name=\"sequence_output\")\n",
    "                bert_outputs = dict(\n",
    "                    sequence_output=sequence_output\n",
    "                )\n",
    "\n",
    "                tf.train.Saver().restore(sess, ckpt_path)\n",
    "\n",
    "                init_op = tf.group([tf.global_variables_initializer(),\n",
    "                                    tf.initializers.tables_initializer(name='init_all_tables')])\n",
    "\n",
    "                sess.run(init_op)\n",
    "\n",
    "                shutil.rmtree(export_dir, ignore_errors=True)\n",
    "\n",
    "                tf.saved_model.simple_save(\n",
    "                    sess,\n",
    "                    export_dir,\n",
    "                    inputs=bert_inputs,\n",
    "                    outputs=bert_outputs,\n",
    "                    legacy_init_op=init_op\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-14 15:41:46--  https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_tensorflow_checkpoint.zip\n",
      "Resolving neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)... 52.219.100.250\n",
      "Connecting to neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)|52.219.100.250|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1205655266 (1,1G) [application/zip]\n",
      "Saving to: ‘bert-base-portuguese-cased_tensorflow_checkpoint.zip’\n",
      "\n",
      "bert-base-portugues 100%[===================>]   1,12G  6,27MB/s    in 9m 20s  \n",
      "\n",
      "2020-09-14 15:51:07 (2,05 MB/s) - ‘bert-base-portuguese-cased_tensorflow_checkpoint.zip’ saved [1205655266/1205655266]\n",
      "\n",
      "Archive:  bert-base-portuguese-cased_tensorflow_checkpoint.zip\n",
      "  inflating: bert-base-portuguese-cased_tensorflow_checkpoint/bert_config.json  \n",
      "  inflating: bert-base-portuguese-cased_tensorflow_checkpoint/model.ckpt.data-00000-of-00001  \n",
      "  inflating: bert-base-portuguese-cased_tensorflow_checkpoint/model.ckpt.index  \n",
      "  inflating: bert-base-portuguese-cased_tensorflow_checkpoint/model.ckpt.meta  \n",
      "--2020-09-14 15:52:25--  https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/vocab.txt\n",
      "Resolving neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)... 52.219.80.24\n",
      "Connecting to neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)|52.219.80.24|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 209528 (205K) [text/plain]\n",
      "Saving to: ‘bert-base-portuguese-cased_tensorflow_checkpoint/vocab.txt’\n",
      "\n",
      "vocab.txt           100%[===================>] 204,62K   394KB/s    in 0,5s    \n",
      "\n",
      "2020-09-14 15:52:28 (394 KB/s) - ‘bert-base-portuguese-cased_tensorflow_checkpoint/vocab.txt’ saved [209528/209528]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's download some BERT Checkpoints\n",
    "!wget https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_tensorflow_checkpoint.zip\n",
    "!unzip bert-base-portuguese-cased_tensorflow_checkpoint.zip -d bert-base-portuguese-cased_tensorflow_checkpoint\n",
    "\n",
    "# For some reason portuguese vocab.txt is not included in the model, \n",
    "# it has to be downloaded separately\n",
    "# most BERT models come with the vocab.txt included\n",
    "!wget -P bert-base-portuguese-cased_tensorflow_checkpoint \"https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_bert(pretrain_path, save_path):\n",
    "\n",
    "    config_path = pretrain_path + '/bert_config.json'\n",
    "    meta_path = pretrain_path + '/model.ckpt.meta'\n",
    "    ckpt_path = pretrain_path + '/model.ckpt'\n",
    "    vocab = pretrain_path + '/vocab.txt'\n",
    "\n",
    "    save_model(config_path, meta_path, ckpt_path, save_path)\n",
    "    os.makedirs(os.path.dirname(save_path+\"/assets/\"), exist_ok=True)\n",
    "    # Spark NLP needs vocab.txt in assets with the same name\n",
    "    copyfile(vocab, save_path+\"/assets/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/camila/Desktop/projetos/ibti/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/camila/Desktop/projetos/ibti/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/camila/Desktop/projetos/ibti/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/camila/Desktop/projetos/ibti/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/camila/Desktop/projetos/ibti/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/camila/Desktop/projetos/ibti/myvenv/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-b48c5f128af9>:51: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "WARNING:tensorflow:From /home/camila/Desktop/projetos/ibti/myvenv/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    }
   ],
   "source": [
    "export_bert('./bert-base-portuguese-cased_tensorflow_checkpoint', './bert_saved_models/bert-base-portuguese-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_ids'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1, -1)\n",
      "        name: input_ids:0\n",
      "    inputs['input_mask'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1, -1)\n",
      "        name: input_mask:0\n",
      "    inputs['segment_ids'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1, -1)\n",
      "        name: segment_ids:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['sequence_output'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, -1, 768)\n",
      "        name: sequence_output:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir ./bert_saved_models/bert-base-portuguese-cased/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_265\"\r\n",
      "OpenJDK Runtime Environment (build 1.8.0_265-8u265-b01-0ubuntu2~18.04-b01)\r\n",
      "OpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install -q pyspark==2.4.6\n",
    "! pip install -q spark-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "spark=sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertEmbeddings.loadSavedModel('./bert_saved_models/bert-base-portuguese-cased/', spark)\\\n",
    " .setInputCols([\"sentence\", \"token\"])\\\n",
    " .setOutputCol(\"bert\")\\\n",
    " .setCaseSensitive(True)\\\n",
    " .setDimension(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.write().save('./BertEmbeddings_bert-base-portuguese-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['sentence'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# you can load an offline model by using .load(PATH)\n",
    "bert = BertEmbeddings.load('./BertEmbeddings_bert-base-portuguese-cased') \\\n",
    " .setInputCols([\"sentence\", \"token\"])\\\n",
    " .setOutputCol(\"bert\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        sentence,\n",
    "        token,\n",
    "        bert        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = spark.createDataFrame([[\"A alemanha é um lugar legal\"]]).toDF(\"text\")\n",
    "\n",
    "prediction = pipeline.fit(prediction_data).transform(prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|result                            |\n",
      "+----------------------------------+\n",
      "|[A, alemanha, é, um, lugar, legal]|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select(\"bert.result\").show(1, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|chunk   |entity|\n",
      "+--------+------+\n",
      "|A       |null  |\n",
      "|alemanha|null  |\n",
      "|é       |null  |\n",
      "|um      |null  |\n",
      "|lugar   |null  |\n",
      "|legal   |null  |\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
